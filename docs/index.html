<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>HDRFlow++: Benchmarking Real-Time HDR Video Reconstruction under Global Motions</title>
  <link rel="stylesheet" href="css/style.css" />
</head>
<body>
    <!-- Hero Section -->
  <header id="hero">
    <h1>HDRFlow++: Benchmarking Real-Time<br />HDR Video Reconstruction under Global Motions</h1>
  
    <!-- NEW: Authors + Team -->
    <p class="authors">
      Jinwoo Lee*, Janghyun Kim*, Seungjae Kim*, Jiheon Park*, Heejin Bin*<br />
      <span class="team">Team P02 路 CS470 KAIST School of Computing</span>
    </p>
  
    <!-- Teaser Image -->
    <figure class="teaser">
      <img src="https://github.com/user-attachments/assets/003a203c-f501-44da-9691-9f781d953d97" alt="Teaser image" />
      <figcaption>Figure 1: <strong>XDRive</strong>, a large-scale synthetic 3D HDR benchmark. Trained and evaluated on XDRive, our proposed HDR video reconstruction model <strong>HDRFlow++</strong> also shows considerable improvements in both HDR reconstruction and downstream vision tasks.</figcaption>
    </figure>
  </header>

  <!-- Abstract Section -->
<!-- Abstract Section -->
  <section id="abstract">
    <h2>Abstract</h2>
    <p>
      Real-time HDR video reconstruction remains challenging under large-scale camera and object motion,
      primarily due to misalignment across multi-exposure frames and the lack of motion-aware fusion strategies.
      Existing methods suffer from degraded reconstruction quality in dynamic environments
      and are often trained on limited datasets with minimal motion diversity.
    </p>
    <p>
      We propose <strong>HDRFlow++</strong>, a motion-robust HDR reconstruction framework that integrates a pretrained RAFT-based feature extractor 
      as a structural prior to guide feature-level alignment and fusion. By leveraging high-quality optical flow in the feature space, 
      our model enhances spatial consistency under severe global motion while maintaining real-time performance.
    </p>
    <p>
      To support training and evaluation, we introduce <strong>XDRive</strong>, a large-scale synthetic 3D HDR benchmark built with CARLA. 
      XDRive provides multimodal ground truth, including HDR frames, aligned multi-exposure LDRs, optical flow, depth maps, and camera poses, captured under diverse lighting and motion conditions. This dataset enables joint optimization for HDR imaging and 3D vision tasks such as monocular depth estimation.
    </p>
    <p>
      Experiments show that <strong>HDRFlow++</strong>, trained with XDRive, achieves up to +2.1 dB improvement in PSNR<sub>T</sub> and consistently better SSIM<sub>T</sub> 
      across local, ego, and global motion scenarios. Additionally, HDR-enhanced inputs improve downstream depth estimation accuracy over standard LDR-based pipelines.
    </p>
    <p>
      Our work bridges HDR reconstruction and 3D perception by introducing a unified training benchmark and a motion-aware fusion model, 
      paving the way for future differentiable visual computing frameworks in dynamic, high-dynamic-range scenes.
    </p>
  </section>

  <!-- Contributions Section -->
  <section id="contributions">
    <h2>Main Contributions</h2>
    <ul>
      <li><strong>HDRFlow++:</strong> Motion-robust HDR reconstruction model with RAFT-enhanced fusion.</li>
      <li><strong>XDRive:</strong> Synthetic 3D HDR dataset containing aligned multi-exposure LDR, GT HDR, depth, and motion ground truth.</li>
      <li><strong>Joint Training Pipeline:</strong> End-to-end differentiable pipeline combining HDR imaging and 3D vision tasks.</li>
      <li><strong>Benchmark Analysis:</strong> Performance evaluation under global, local, and ego motion using PSNR<sub>T</sub> and SSIM<sub>T</sub>.</li>
    </ul>
  </section>

  <!-- Results Section -->
  <section id="results">
    <h2>Qualitative Results</h2>
    <div class="grid">
      <figure>
        <img src="assets/results/global_motion.gif" />
        <figcaption>Scene 1: HDR reconstruction under global motion (Vanilla HDRFlow)</figcaption>
      </figure>
      <figure>
        <img src="assets/results/depth_estimation.gif" />
        <figcaption>Scene 2: HDR reconstruction under global motion (Fine-tuned HDRFlow++)</figcaption>
      </figure>
    </div>
  </section>

  <!-- Demo Section -->
  <section id="demo">
    <h2>Interactive Demo</h2>
    <canvas id="viewer"></canvas>
    <p class="note">Monocular depth estimation (UniDepthV2) improves using reconsturcted HDR input</p>
  </section>

  <!-- BibTeX + Links -->
  <section id="resources">
    <h2>BibTeX</h2>
    <pre>
@misc{lee2025hdrflowpp,
  title={HDRFlow++: Benchmarking HDR Reconstruction under Global Motions},
  author={Lee, Jinwoo and Kim, Janghyun and Kim, Seungjae and Park, Jiheon and Bin, Heejin},
  howpublished={CS470 Project, KAIST},
  year={2025}
}
    </pre>
    <p>
      <a href="https://drive.google.com/file/d/18_Ip8usMrJoTjKBHDozhMRNvL6L2hAhg/view?usp=share_link">[Poster]</a> 路
      <a href="https://github.com/cinescope-wkr/XDRive">[Code]</a> 路
      <a href="https://drive.google.com/drive/folders/1l31fy8DaN4-H1oFoLqq5kpuk5f7iFIq_?usp=share_link">[Dataset]</a> 路
      <a href="https://jak-cal.github.io/XDRive_docs/">[Project Page]</a>
    </p>
  </section>

  <script src="js/main.js"></script>
</body>
</html>
