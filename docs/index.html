<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>HDRFlow++: Real-Time HDR Video Benchmarking</title>
  <link rel="stylesheet" href="css/style.css" />
</head>
<body>
  <!-- Hero Section -->
  <header id="hero">
    <video src="assets/teaser_hdrflow.mp4" autoplay loop muted playsinline></video>
    <h1>HDRFlow++: Benchmarking Real-Time  HDR Video Reconstruction under Global Motions</h1>
    <p class="subtitle">SIGGRAPH Asia 2021 · Park et al.</p>
  </header>

  <!-- Abstract Section -->
  <section id="abstract">
    <h2>Abstract</h2>
    <p>
      We introduce <strong>HDRFlow++</strong>, a real-time HDR video reconstruction model designed to handle global motions by incorporating a pretrained RAFT-based feature extractor as a structural prior. 
      To enable training and evaluation under large-scale scene changes, we also present <strong>XDRive</strong>, a synthetic 3D HDR benchmark with multimodal ground truth—encompassing HDR frames, depth, optical flow, and camera poses.
    </p>
    <p>
      Our framework supports joint training for <strong>HDR imaging</strong> and <strong>3D perception tasks</strong> such as monocular depth estimation.
      We show that models trained with XDRive exhibit superior robustness to camera and object motion, outperforming existing HDR reconstruction pipelines across various motion magnitudes.
    </p>
  </section>

  <!-- Contributions Section -->
  <section id="contributions">
    <h2>Main Contributions</h2>
    <ul>
      <li><strong>HDRFlow++:</strong> Motion-robust HDR reconstruction model with RAFT-enhanced fusion.</li>
      <li><strong>XDRive:</strong> Synthetic 3D HDR dataset containing aligned multi-exposure LDR, GT HDR, depth, and motion ground truth.</li>
      <li><strong>Joint Training Pipeline:</strong> End-to-end differentiable pipeline combining HDR imaging and 3D vision tasks.</li>
      <li><strong>Benchmark Analysis:</strong> Performance evaluation under global, local, and ego motion using PSNR<sub>T</sub> and SSIM<sub>T</sub>.</li>
    </ul>
  </section>

  <!-- Results Section -->
  <section id="results">
    <h2>Qualitative Results</h2>
    <div class="grid">
      <figure>
        <img src="assets/results/global_motion.gif" />
        <figcaption>Scene 1: HDR reconstruction under global motion (HDRFlow++)</figcaption>
      </figure>
      <figure>
        <img src="assets/results/depth_estimation.gif" />
        <figcaption>Scene 2: Monocular depth estimation improves using HDR input</figcaption>
      </figure>
    </div>
  </section>

  <!-- Demo Section -->
  <section id="demo">
    <h2>Interactive Demo</h2>
    <canvas id="viewer"></canvas>
    <p class="note">View reconstructed HDR frame and 3D perception overlay</p>
  </section>

  <!-- BibTeX + Links -->
  <section id="resources">
    <h2>BibTeX</h2>
    <pre>
@misc{lee2025hdrflowpp,
  title={HDRFlow++: Benchmarking HDR Reconstruction under Global Motions},
  author={Lee, Jinwoo and Kim, Janghyun and Kim, Seungjae and Park, Jiheon and Bin, Heejin},
  howpublished={CS470 Project, KAIST},
  year={2025}
}
    </pre>
    <p>
      <a href="https://arxiv.org/abs/XXXX.XXXXX">[Paper]</a> ·
      <a href="https://github.com/CS470Team02/HDRFlowPlusPlus">[Code]</a> ·
      <a href="https://your-custom-project-site-link">[Project Page]</a>
    </p>
  </section>

  <script src="js/main.js"></script>
</body>
</html>
