<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>HDRFlow++: Benchmarking Real-Time HDR Video Reconstruction under Global Motions</title>
  <link rel="stylesheet" href="css/style.css" />
</head>
<body>
    <!-- Hero Section -->
  <header id="hero">
    <h1>HDRFlow++: Benchmarking Real-Time<br />HDR Video Reconstruction under Global Motions</h1>
  
    <!-- NEW: Authors + Team -->
    <p class="authors">
      Jinwoo Lee*, Janghyun Kim*, Seungjae Kim*, Jiheon Park*, Heejin Bin*<br />
      <span class="team">Team P02 路 CS470 KAIST School of Computing</span>
    </p>
  
    <!-- Teaser Image -->
    <figure class="teaser">
      <img src="https://github.com/user-attachments/assets/003a203c-f501-44da-9691-9f781d953d97" alt="Teaser image" />
      <figcaption>Figure 1: <strong>XDRive</strong>, a large-scale synthetic 3D HDR benchmark. Trained and evaluated on XDRive, our proposed HDR video reconstruction model <strong>HDRFlow++</strong> also shows considerable improvements in both HDR reconstruction and downstream vision tasks.</figcaption>
    </figure>
  </header>

  <!-- Abstract Section -->
<!-- Abstract Section -->
  <section id="abstract">
    <h2>Abstract</h2>
    <p>
      Real-time HDR video reconstruction remains challenging under large-scale camera and object motion,
      primarily due to misalignment across multi-exposure frames and the lack of motion-aware fusion strategies.
      Existing methods suffer from degraded reconstruction quality in dynamic environments
      and are often trained on limited datasets with minimal motion diversity.
    </p>
    <p>
      We propose <strong>HDRFlow++</strong>, a motion-robust HDR reconstruction framework that integrates a pretrained RAFT-based feature extractor 
      as a structural prior to guide feature-level alignment and fusion. By leveraging high-quality optical flow in the feature space, 
      our model enhances spatial consistency under severe global motion while maintaining real-time performance.
    </p>
    <p>
      To support training and evaluation, we introduce <strong>XDRive</strong>, a large-scale synthetic 3D HDR benchmark built with CARLA. 
      XDRive provides multimodal ground truth, including HDR frames, aligned multi-exposure LDRs, optical flow, depth maps, and camera poses, captured under diverse lighting and motion conditions. This dataset enables joint optimization for HDR imaging and 3D vision tasks such as monocular depth estimation.
    </p>
    <p>
      Experiments show that <strong>HDRFlow++</strong>, trained with XDRive, achieves up to +2.1 dB improvement in PSNR<sub>T</sub> and consistently better SSIM<sub>T</sub> 
      across local, ego, and global motion scenarios. Additionally, HDR-enhanced inputs improve downstream depth estimation accuracy over standard LDR-based pipelines.
    </p>
    <p>
      Our work bridges HDR reconstruction and 3D perception by introducing a unified training benchmark and a motion-aware fusion model, 
      paving the way for future differentiable visual computing frameworks in dynamic, high-dynamic-range scenes. The benchmark code and dataset will be publicly available to facilitate reproducibility and future research.
    </p>
  </section>

  <!-- Contributions Section -->
  <section id="contributions">
    <h2>Main Contributions</h2>
    <ul>
      <li><strong>HDRFlow++:</strong> Motion-robust HDR reconstruction model with RAFT-enhanced fusion.</li>
      <li><strong>XDRive:</strong> Synthetic 3D HDR dataset containing aligned multi-exposure LDR, GT HDR, depth, and motion ground truth.</li>
      <li><strong>Joint Training Pipeline:</strong> End-to-end differentiable pipeline combining HDR imaging and 3D vision tasks.</li>
      <li><strong>Benchmark Analysis:</strong> Performance evaluation under global, local, and ego motion using PSNR<sub>T</sub> and SSIM<sub>T</sub>.</li>
    </ul>
  </section>


  <!-- Demo Section -->
  <section id="benchmark">
    <h2>3D HDR Imaging Benchmark Design</h2>
    <figure class="benchmark-figure">
      <img src="https://github.com/user-attachments/assets/17d71601-ced5-4589-96aa-a7e10a2e2d7e" 
           alt="Benchmark overview: HDR imaging under dynamic motion scenarios" />
      <figcaption>
        <strong>Figure 2:</strong> Benchmark overview of our evaluation and training scheme. The HDR dataset can be simulated as LDR via an image capture simulator by adjusting exposure and adding noise. This is then trained and tested on HDR video reconstruction, with 3D tasks benefitting from ground truth 3D HDR data.
      </figcaption>
    </figure>
  </section>
  

  <section id="results">
    <h2>Qualitative Results</h2>
    <div class="grid">
      <figure>
        <img src="https://github.com/user-attachments/assets/79b0cdc7-b99f-478d-9827-0d945f54801d"
             alt="Baseline HDRFlow result under global motion" />
        <figcaption>
          Scene 1: HDR reconstruction result under global motion using <strong>Vanilla HDRFlow</strong>. Ghosting artifacts are clearly visible.
        </figcaption>
      </figure>
      
      <figure>
        <img src="https://github.com/user-attachments/assets/c1686541-f02b-4998-b546-0d8b9879348a"
             alt="HDRFlow++ result under global motion" />
        <figcaption>
          Scene 2: HDR reconstruction result using <strong>HDRFlow++</strong>. Feature-guided fusion improves alignment under global motion.
        </figcaption>
      </figure>
    </div>
  </section>

  <section id="demo">
    <h2>Interactive Demo: Finetuned HDRFlow++</h2>
    <div class="grid">
      <figure>
        <img src="https://github.com/user-attachments/assets/544ed507-f8ac-4cd2-84b6-fb860d9514a8"
             alt="Alternating exposure input frames (LDR)" />
        <figcaption>
          <strong>Input:</strong> Alternating exposure input frames (LDR). 
        </figcaption>
      </figure>
  
      <figure>
        <img src="https://github.com/user-attachments/assets/a44c18bf-924e-4610-8321-5f5a68d19dc0"
             alt="HDRFlow++ reconstructed HDR output frames" />
        <figcaption>
          <strong>Output:</strong> Reconstructed HDR frames using <strong>finetuned HDRFlow++</strong>. Feature-guided alignment improves temporal consistency and reduces ghosting.
        </figcaption>
      </figure>
    </div>
  
    <p class="note">
      Our model integrates optical flow priors to align alternating exposure inputs at the feature level, enabling real-time HDR reconstruction under challenging global motion.
    </p>
  </section>



  <section id="perception-demo">
  <h2>3D Perception Demo: Monocular Depth Estimation</h2>
  <figure class="perception-figure">
    <img src="https://github.com/user-attachments/assets/f97edb86-5c30-42b5-ad4b-64f74692e88f"
         alt="Monocular depth estimation using HDR input" />
    <figcaption>
      <strong>Figure 5:</strong> The proposed 3D HDR dataset provides ground truth depth data. Inference using reconstructed HDR images showed clear improvements in depth estimation accuracy. It opens up possibilities for future differentiable computational imaging algorithms.
    </figcaption>
  </figure>
</section>

  <!-- BibTeX + Links -->
  <section id="resources">
    <h2>BibTeX</h2>
    <pre>
@misc{lee2025hdrflowpp,
  title={HDRFlow++: Benchmarking HDR Reconstruction under Global Motions},
  author={Lee, Jinwoo and Kim, Janghyun and Kim, Seungjae and Park, Jiheon and Bin, Heejin},
  howpublished={CS470 Project, KAIST},
  year={2025}
}
    </pre>
    <p>
      <a href="https://drive.google.com/file/d/18_Ip8usMrJoTjKBHDozhMRNvL6L2hAhg/view?usp=share_link">[Poster]</a> 路
      <a href="https://github.com/cinescope-wkr/XDRive">[Code]</a> 路
      <a href="https://drive.google.com/drive/folders/1l31fy8DaN4-H1oFoLqq5kpuk5f7iFIq_?usp=share_link">[Dataset]</a> 路
      <a href="https://jak-cal.github.io/XDRive_docs/">[Project Page]</a>
    </p>
  </section>

  <script src="js/main.js"></script>
</body>
</html>
